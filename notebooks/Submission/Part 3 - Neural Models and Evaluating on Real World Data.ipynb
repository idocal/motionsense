{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> MotiononSense Dataset : Smartphone Sensor Data </h1>\n",
    "<h3> Problem definition - predict user's activity based on smartphone sensors data </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Part 3:\n",
    "<ul>\n",
    "    <li> Extracting real world data </li>\n",
    "    <li> Evaluation on real world data </li>\n",
    "    <li> Neuronal Models - training and evaluation on real world data </li>\n",
    "    <li> Final results, conculsions and application </li>\n",
    "    </ul>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Extracting real world data </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> We used the <a href=\"https://developer.apple.com/documentation/coremotion/cmdevicemotion\">Core Motion Framework for iOS devices</a> to extract sensors data from our phones </li>\n",
    "    <li> More details on the app we built can be found in our final document </li>\n",
    "    <li> We recorded sensors data while performing different activties and extracted labeled data samples </li>\n",
    "    <li> On the next section we will load our data and use it as a test set to evaluate the performace of our Random Forest model </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class NewDataLoader():\n",
    "    \n",
    "    def __init__(self, folder_path):\n",
    "        self.data_path = folder_path\n",
    "    \n",
    "    def load_all_expirements(self):\n",
    "        df = None\n",
    "        exp_index = 1\n",
    "        for filename in os.listdir(self.data_path):\n",
    "            file_path = os.path.join(self.data_path, filename)\n",
    "            extension = os.path.splitext(file_path)[1]\n",
    "            if extension == '.csv':\n",
    "                current_df = self.load_single_test_expirement(file_path, exp_index)\n",
    "                exp_index += 1\n",
    "                if df is None:\n",
    "                    df = current_df\n",
    "                else:\n",
    "                    df = df.append(current_df)\n",
    "        return df\n",
    "\n",
    "    def load_single_test_expirement(self, path_to_file, exp_index, partc_id=1):\n",
    "        cols_to_drop = [\"timestamp\", \"timeIntervalSince1970\", 'magneticField.x', \n",
    "                        'magneticField.y', 'magneticField.z', 'magneticField.accuracy']\n",
    "        file_name = path_to_file.split(os.sep)[-1]\n",
    "        name, file_type = file_name.split('.')\n",
    "        action = name[:3]\n",
    "        exp_df = pd.read_csv(path_to_file)\n",
    "        exp_df = exp_df.drop(cols_to_drop, axis=1)\n",
    "        exp_df[\"partc\"] = partc_id\n",
    "        exp_df[\"action\"] = action\n",
    "        exp_df[\"action_file_index\"] = exp_index\n",
    "        return exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_MAIN_DIR = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "path = os.path.join(PROJECT_MAIN_DIR, 'real-data')\n",
    "data_loader = NewDataLoader(path)\n",
    "real_test_df = data_loader.load_all_expirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load also our original data set and use it as a training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PROJECT_MAIN_DIR,'full_data.gz'), compression='gzip') # we will load our data saved as a compressed csv file\n",
    "train_df = train_df.drop(['Unnamed: 0'], axis=1).set_index('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Evaluation real world data </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will encode both samples with our Sliding Window encoding, train our Random Forest model over the entire old data and evaluate it's performance on the real world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindow:\n",
    "    \n",
    "    def __init__(self, orig_df, window_size, num_experiments, num_participants, exclude, fnlist):\n",
    "        exps = [i for i in range(1,num_experiments + 1) if i != exclude]\n",
    "        parts = [i for i in range(1,num_participants + 1)]\n",
    "        smp_df = self.create_sliding_df(orig_df, window_size, fnlist, exps, parts)\n",
    "        self.window_size = window_size\n",
    "        self.df = smp_df\n",
    "\n",
    "    def create_sld_df_single_exp(self, orig_df, window_size, analytic_functions_list):\n",
    "        dfs_to_concate = []\n",
    "        base_df = orig_df.drop('action', axis=1)\n",
    "        for func in analytic_functions_list:\n",
    "            method_to_call = getattr(base_df.rolling(window=window_size), func)\n",
    "            analytic_df = method_to_call()\n",
    "            analytic_df = analytic_df[window_size:]\n",
    "            analytic_df.columns = [col + \"_sld_\" + func for col in analytic_df.columns]\n",
    "            dfs_to_concate.append(analytic_df)\n",
    "\n",
    "        action_df = orig_df[['action']][window_size:] # [[]] syntax to return DataFrame and not Series\n",
    "        dfs_to_concate.append(action_df)\n",
    "        return pd.concat(dfs_to_concate,axis=1)\n",
    "\n",
    "    def create_sliding_df(self, orig_df, window_size, analytic_functions_list, expirements, participants):\n",
    "        dfs_to_concate = []\n",
    "        cols_to_drop = ['partc', 'action_file_index']\n",
    "        for e in expirements:\n",
    "            for p in participants:\n",
    "                exp_df = orig_df[(orig_df['partc'] == p) & (orig_df['action_file_index'] == e)]\n",
    "                exp_df = exp_df.drop(cols_to_drop, axis=1)\n",
    "                exp_roll_df = self.create_sld_df_single_exp(exp_df, window_size, analytic_functions_list)\n",
    "\n",
    "                dfs_to_concate.append(exp_roll_df)\n",
    "        return pd.concat(dfs_to_concate, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variables for the sliding window data frame creation\n",
    "num_experiments = 16\n",
    "num_participants = 24\n",
    "exclude = 10\n",
    "analytic_functions_list = ['mean', 'sum', 'median', 'min', 'max', 'std']\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "# create the sliding window data frame\n",
    "train_win_df = SlidingWindow(train_df, WINDOW_SIZE, num_experiments, num_participants, exclude, analytic_functions_list)\n",
    "train_win_df = train_win_df.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 18\n",
    "num_participants = 1\n",
    "exclude = 0\n",
    "analytic_functions_list = ['mean', 'sum', 'median', 'min', 'max', 'std']\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "real_test_df[\"partc\"] = 1\n",
    "test_win_df = SlidingWindow(real_test_df, WINDOW_SIZE, num_experiments, num_participants, exclude, analytic_functions_list)\n",
    "test_win_df = test_win_df.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class DataProcessingEval():\n",
    "    \n",
    "    def __init__(self, origin_df, labels_dict):\n",
    "        self.labels_dict = labels_dict\n",
    "        self.classes_names = self.create_classes(labels_dict)\n",
    "        self.df = origin_df\n",
    "    \n",
    "    def create_samples(self, division_ratio=[0.7, 0.1, 0.2]):\n",
    "        # Define X, y\n",
    "        df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        X, y = df.drop([\"action\"], axis=1), df[\"action\"]\n",
    "        y = y.replace(self.labels_dict)\n",
    "\n",
    "        # Divide to training, validation and test set\n",
    "        train_ratio, dev_ratio = division_ratio[0], division_ratio[1]\n",
    "        num_training = int(df.shape[0] * train_ratio)\n",
    "        num_validation = int(df.shape[0] * dev_ratio)\n",
    "        \n",
    "        X_train, y_train = X[:num_training], y[:num_training]\n",
    "        X_vald, y_vald = X[num_training:num_training + num_validation], y[num_training:num_training + num_validation]\n",
    "        X_test, y_test = X[num_training + num_validation:], y[num_training + num_validation:]\n",
    "\n",
    "        return X_train, y_train, X_vald, y_vald, X_test, y_test\n",
    "\n",
    "    def create_classes(self, labels_dict):\n",
    "        classes_indexs = labels_dict.items()\n",
    "        classes_indexs = sorted(classes_indexs, key=lambda x: x[1])\n",
    "        classes_names = [label for label, index in classes_indexs]\n",
    "        return classes_names\n",
    "\n",
    "    def evaluate_results(self, y_true, y_pred):\n",
    "            print(\"---- Printing classification report ----\")\n",
    "            print(classification_report(y_true, y_pred, target_names=self.classes_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'wlk': 0, 'sit': 1, \"std\": 2, \"ups\": 3, \"jog\": 4, \"dws\": 5}\n",
    "\n",
    "win_train_processor = DataProcessingEval(train_win_df, labels_dict=labels)\n",
    "X_train, y_train, _, _, _, _  = win_train_processor.create_samples([1.0, 0, 0])\n",
    "\n",
    "win_test_processor = DataProcessingEval(test_win_df, labels_dict=labels)\n",
    "X_test_real, y_test_real, _, _, _, _ = win_test_processor.create_samples([1.0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training over the entire original data and evaluating on new test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   58.5s remaining:   39.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Printing classification report ----\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        wlk       0.78      0.47      0.59     52652\n",
      "        sit       0.97      0.70      0.81     35225\n",
      "        std       0.96      0.97      0.96     36561\n",
      "        ups       0.43      0.75      0.54     21800\n",
      "        jog       0.00      0.00      0.00         0\n",
      "        dws       0.27      0.45      0.34     19547\n",
      "\n",
      "avg / total       0.75      0.66      0.68    165785\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.2s finished\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=10, n_jobs=-1, verbose=1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_test_predictions = rf.predict(X_test_real)\n",
    "win_test_processor.evaluate_results(y_test_real, rf_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions so far:\n",
    "<ul>\n",
    "    <li> We excluded the \"jogging\" activity beacuse we didn't preform this activity in the data we created from our app </li>\n",
    "    <li> As predicted, the results on real world data are much worse compared to results over our original test set </li>\n",
    "    <li> We are still predicting \"sit\" and \"stand\" activities quite well but our current model is having hard time identifying \"upstairs\" and \"down stairs\" </li>\n",
    "    <li> Next, we will try to use a stronger, neural models, hoping that it will help us increasing our performance over the real test data\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Neural Models - Training and Evaluation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding for Neural Models\n",
    "<ul>\n",
    "    <li> The first model we will try is a simple feed forward network with one hidden layer </li>\n",
    "    <li> Feed forward nets, like classic ML models, cannot use sequence as input so we will have to use one of our previous encodings </li>\n",
    "    <li> We will choose our sliding window encoding first, since it out-performed our raw history encoding </li>\n",
    "    <li> We hope that our model can creat a better representation of the data in it's hidden layer and thus increase the generalization ability of the model </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1409265, 73)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_win_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_one_hot(index, array_size):\n",
    "    one_hot = np.array([0 for _ in range(array_size)])\n",
    "    one_hot[index] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "ff_model = Sequential()\n",
    "ff_model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))  # hidden layer size is 32\n",
    "ff_model.add(Dropout(dropout_rate))  # adding dropout layer\n",
    "ff_model.add(Dense(6, activation='softmax'))  # applying softmax and cross entorpy loss\n",
    "ff_model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1409265/1409265 [==============================] - 84s 60us/step - loss: 0.5534\n",
      "Epoch 2/5\n",
      "1409265/1409265 [==============================] - 82s 58us/step - loss: 0.4995\n",
      "Epoch 3/5\n",
      "1409265/1409265 [==============================] - 83s 59us/step - loss: 0.4926\n",
      "Epoch 4/5\n",
      "1409265/1409265 [==============================] - 83s 59us/step - loss: 0.4888\n",
      "Epoch 5/5\n",
      "1409265/1409265 [==============================] - 81s 57us/step - loss: 0.4857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1189340b8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "num_activities = 6\n",
    "y_train_one_hot = np.array([to_categorical(t, num_activities) for t in y_train])\n",
    "\n",
    "# tranform y to one hot encoding vector of length 6 (we have 6 activities)\n",
    "ff_model.fit(X_train, y_train_one_hot, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165785/165785 [==============================] - 3s 16us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9681791116666617"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_real_one_hot = np.array([index_to_one_hot(index, num_activities) for index in y_test_real])\n",
    "ff_model.evaluate(X_test_real, y_test_real_one_hot, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
