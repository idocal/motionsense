{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MotiononSense Dataset\n",
    "### Problem definition: predict user's activity based on smartphone sensors data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3:\n",
    "\n",
    "* Extracting Real World Data \n",
    "* Evaluation on Real World Data \n",
    "* Neural Models - Training and Evaluation on Real World Data\n",
    "* Applicative Predictions Smoothing\n",
    "* Final Results, Conculsions & Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Extracting Real World Data\n",
    "\n",
    "\n",
    "* We used the [Core Motion Framework for iOS devices](https://developer.apple.com/documentation/coremotion/cmdevicemotion) to extract sensors data from our phones\n",
    "* More details on the app we built can be found in our final document\n",
    "* We recorded sensors data while performing different activties and extracted labeled data samples \n",
    "* On the next section we will load our data and use it as a test set to evaluate the performace of our Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "class NewDataLoader():\n",
    "    \n",
    "    def __init__(self, folder_path):\n",
    "        self.data_path = folder_path\n",
    "    \n",
    "    def load_all_expirements(self):\n",
    "        df = None\n",
    "        exp_index = 1\n",
    "        for filename in os.listdir(self.data_path):\n",
    "            file_path = os.path.join(self.data_path, filename)\n",
    "            extension = os.path.splitext(file_path)[1]\n",
    "            if extension == '.csv':\n",
    "                current_df = self.load_single_test_expirement(file_path, exp_index)\n",
    "                exp_index += 1\n",
    "                if df is None:\n",
    "                    df = current_df\n",
    "                else:\n",
    "                    df = df.append(current_df)\n",
    "        return df\n",
    "\n",
    "    def load_single_test_expirement(self, path_to_file, exp_index, partc_id=1):\n",
    "        cols_to_drop = [\"timestamp\", \"timeIntervalSince1970\", 'magneticField.x', \n",
    "                        'magneticField.y', 'magneticField.z', 'magneticField.accuracy']\n",
    "        file_name = path_to_file.split(os.sep)[-1]\n",
    "        name, file_type = file_name.split('.')\n",
    "        action = name[:3]\n",
    "        exp_df = pd.read_csv(path_to_file)\n",
    "        exp_df = exp_df.drop(cols_to_drop, axis=1)\n",
    "        exp_df[\"partc\"] = partc_id\n",
    "        exp_df[\"action\"] = action\n",
    "        exp_df[\"action_file_index\"] = exp_index\n",
    "        return exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_MAIN_DIR = os.getcwd()\n",
    "path = os.path.join(PROJECT_MAIN_DIR, 'real-data')\n",
    "data_loader = NewDataLoader(path)\n",
    "real_test_df = data_loader.load_all_expirements()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load also our original data set and use it as a training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(PROJECT_MAIN_DIR,'full_data.gz'), compression='gzip') # we will load our data saved as a compressed csv file\n",
    "train_df = train_df.drop(['Unnamed: 0'], axis=1).set_index('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluation on Real World Data\n",
    "\n",
    "Now, we will encode both samples with our Sliding Window encoding, train our Random Forest model over the entire old data and evaluate it's performance on the real world data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindow:\n",
    "    \n",
    "    def __init__(self, orig_df, window_size, num_experiments, num_participants, exclude, fnlist):\n",
    "        exps = [i for i in range(1,num_experiments + 1) if i != exclude]\n",
    "        parts = [i for i in range(1,num_participants + 1)]\n",
    "        smp_df = self.create_sliding_df(orig_df, window_size, fnlist, exps, parts)\n",
    "        self.window_size = window_size\n",
    "        self.df = smp_df\n",
    "\n",
    "    def create_sld_df_single_exp(self, orig_df, window_size, analytic_functions_list):\n",
    "        dfs_to_concate = []\n",
    "        base_df = orig_df.drop('action', axis=1)\n",
    "        for func in analytic_functions_list:\n",
    "            method_to_call = getattr(base_df.rolling(window=window_size), func)\n",
    "            analytic_df = method_to_call()\n",
    "            analytic_df = analytic_df[window_size:]\n",
    "            analytic_df.columns = [col + \"_sld_\" + func for col in analytic_df.columns]\n",
    "            dfs_to_concate.append(analytic_df)\n",
    "\n",
    "        action_df = orig_df[['action']][window_size:] # [[]] syntax to return DataFrame and not Series\n",
    "        dfs_to_concate.append(action_df)\n",
    "        return pd.concat(dfs_to_concate,axis=1)\n",
    "\n",
    "    def create_sliding_df(self, orig_df, window_size, analytic_functions_list, expirements, participants):\n",
    "        dfs_to_concate = []\n",
    "        cols_to_drop = ['partc', 'action_file_index']\n",
    "        for e in expirements:\n",
    "            for p in participants:\n",
    "                exp_df = orig_df[(orig_df['partc'] == p) & (orig_df['action_file_index'] == e)]\n",
    "                exp_df = exp_df.drop(cols_to_drop, axis=1)\n",
    "                exp_roll_df = self.create_sld_df_single_exp(exp_df, window_size, analytic_functions_list)\n",
    "\n",
    "                dfs_to_concate.append(exp_roll_df)\n",
    "        return pd.concat(dfs_to_concate, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variables for the sliding window data frame creation\n",
    "num_experiments = 16\n",
    "num_participants = 24\n",
    "exclude = 10\n",
    "analytic_functions_list = ['mean', 'sum', 'median', 'min', 'max', 'std']\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "# create the sliding window data frame\n",
    "train_win_df = SlidingWindow(train_df, WINDOW_SIZE, num_experiments, num_participants, exclude, analytic_functions_list)\n",
    "train_win_df = train_win_df.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 18\n",
    "num_participants = 1\n",
    "exclude = 0\n",
    "analytic_functions_list = ['mean', 'sum', 'median', 'min', 'max', 'std']\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "real_test_df[\"partc\"] = 1\n",
    "test_win_df = SlidingWindow(real_test_df, WINDOW_SIZE, num_experiments, num_participants, exclude, analytic_functions_list)\n",
    "test_win_df = test_win_df.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class DataProcessingEval():\n",
    "    \n",
    "    def __init__(self, origin_df, labels_dict):\n",
    "        self.labels_dict = labels_dict\n",
    "        self.classes_names = self.create_classes(labels_dict)\n",
    "        self.df = origin_df\n",
    "    \n",
    "    def create_samples(self, division_ratio=[0.7, 0.1, 0.2]):\n",
    "        # Define X, y\n",
    "        df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        X, y = df.drop([\"action\"], axis=1), df[\"action\"]\n",
    "        y = y.replace(self.labels_dict)\n",
    "\n",
    "        # Divide to training, validation and test set\n",
    "        train_ratio, dev_ratio = division_ratio[0], division_ratio[1]\n",
    "        num_training = int(df.shape[0] * train_ratio)\n",
    "        num_validation = int(df.shape[0] * dev_ratio)\n",
    "        \n",
    "        X_train, y_train = X[:num_training], y[:num_training]\n",
    "        X_vald, y_vald = X[num_training:num_training + num_validation], y[num_training:num_training + num_validation]\n",
    "        X_test, y_test = X[num_training + num_validation:], y[num_training + num_validation:]\n",
    "\n",
    "        return X_train, y_train, X_vald, y_vald, X_test, y_test\n",
    "\n",
    "    def create_classes(self, labels_dict):\n",
    "        classes_indexs = labels_dict.items()\n",
    "        classes_indexs = sorted(classes_indexs, key=lambda x: x[1])\n",
    "        classes_names = [label for label, index in classes_indexs]\n",
    "        return classes_names\n",
    "\n",
    "    def evaluate_results(self, y_true, y_pred):\n",
    "            print(\"---- Printing classification report ----\")\n",
    "            print(classification_report(y_true, y_pred, target_names=self.classes_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'wlk': 0, 'sit': 1, \"std\": 2, \"ups\": 3, \"jog\": 4, \"dws\": 5}\n",
    "\n",
    "win_train_processor = DataProcessingEval(train_win_df, labels_dict=labels)\n",
    "X_train, y_train, _, _, _, _  = win_train_processor.create_samples([1.0, 0, 0])\n",
    "\n",
    "win_test_processor = DataProcessingEval(test_win_df, labels_dict=labels)\n",
    "X_test_real, y_test_real, _, _, _, _ = win_test_processor.create_samples([1.0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training over the entire original data and evaluating on new test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of  10 | elapsed:   48.1s remaining:   32.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Printing classification report ----\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        wlk       0.65      0.48      0.55     52652\n",
      "        sit       0.98      0.69      0.81     35225\n",
      "        std       0.96      0.96      0.96     36561\n",
      "        ups       0.40      0.74      0.52     21800\n",
      "        jog       0.00      0.00      0.00         0\n",
      "        dws       0.40      0.45      0.42     19547\n",
      "\n",
      "avg / total       0.72      0.66      0.68    165785\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   6 out of  10 | elapsed:    0.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=8)]: Done  10 out of  10 | elapsed:    0.1s finished\n",
      "/Users/okleinfeld/venv/ds_workshop/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=10, n_jobs=-1, verbose=1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_test_predictions = rf.predict(X_test_real)\n",
    "win_test_processor.evaluate_results(y_test_real, rf_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions so far:**\n",
    "\n",
    "* We excluded the \"jogging\" activity because we didn't perform this activity in the data we created from our app\n",
    "* As predicted, the results on real world data are much worse compared to results over our original test set\n",
    "* We are still predicting \"sit\" and \"stand\" activities quite well but our current model is having hard time identifying \"upstairs\" and \"down stairs\"\n",
    "* Next, we will try to use a stronger, neural models, hoping that it will help us increasing our performance over the real test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Neural Models - Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding for Neural Models**\n",
    "\n",
    "* The first model we will try is a simple feed forward network with one hidden layer\n",
    "* Feed forward nets, like classic ML models, cannot use sequence as input so we will have to use one of our previous encodings \n",
    "* We will choose our sliding window encoding first, since it out-performed our raw history encoding\n",
    "* We hope that our model can create a better representation of the data in it's hidden layer and thus increase the generalization ability of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1409265, 73)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_win_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "dropout_rate = 0.5\n",
    "\n",
    "ff_model = Sequential()\n",
    "ff_model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))  # hidden layer size is 32\n",
    "ff_model.add(Dropout(dropout_rate))  # adding dropout layer\n",
    "ff_model.add(Dense(6, activation='softmax'))  # applying softmax and cross entorpy loss\n",
    "ff_model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1409265/1409265 [==============================] - 43s 31us/step - loss: 0.5436\n",
      "Epoch 2/5\n",
      "1409265/1409265 [==============================] - 44s 31us/step - loss: 0.4918\n",
      "Epoch 3/5\n",
      "1409265/1409265 [==============================] - 42s 30us/step - loss: 0.4860\n",
      "Epoch 4/5\n",
      "1409265/1409265 [==============================] - 45s 32us/step - loss: 0.4816\n",
      "Epoch 5/5\n",
      "1409265/1409265 [==============================] - 44s 32us/step - loss: 0.4799\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11e5bcda0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "num_activities = 6\n",
    "y_train_one_hot = np.array([to_categorical(t, num_activities) for t in y_train])\n",
    "\n",
    "# tranform y to one hot encoding vector of length 6 (we have 6 activities)\n",
    "ff_model.fit(X_train, y_train_one_hot, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Printing classification report ----\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        wlk       0.80      0.52      0.63     52652\n",
      "        sit       0.88      0.97      0.92     35225\n",
      "        std       0.89      0.97      0.93     36561\n",
      "        ups       0.32      0.66      0.43     21800\n",
      "        jog       0.00      0.00      0.00         0\n",
      "        dws       0.39      0.16      0.23     19547\n",
      "\n",
      "avg / total       0.73      0.69      0.68    165785\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okleinfeld/venv/ds_workshop/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_test_real_one_hot = np.array([to_categorical(t, num_activities) for t in y_test_real])\n",
    "ff_predictions = ff_model.predict(X_test_real)\n",
    "ff_test_predictions = np.array([np.argmax(prediction) for prediction in ff_predictions])\n",
    "win_test_processor.evaluate_results(y_test_real, ff_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate results:**\n",
    "* Results for Feed Forward neural network look like the Random Forest ones\n",
    "* Try to add another hidden layer and see if significant improvement occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1409265/1409265 [==============================] - 50s 35us/step - loss: 0.4480\n",
      "Epoch 2/5\n",
      "1409265/1409265 [==============================] - 49s 35us/step - loss: 0.3645\n",
      "Epoch 3/5\n",
      "1409265/1409265 [==============================] - 47s 33us/step - loss: 0.3465\n",
      "Epoch 4/5\n",
      "1409265/1409265 [==============================] - 46s 33us/step - loss: 0.3352\n",
      "Epoch 5/5\n",
      "1409265/1409265 [==============================] - 47s 34us/step - loss: 0.3287\n",
      "---- Printing classification report ----\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        wlk       0.84      0.46      0.59     52652\n",
      "        sit       0.94      0.97      0.96     35225\n",
      "        std       0.95      0.97      0.96     36561\n",
      "        ups       0.42      0.74      0.54     21800\n",
      "        jog       0.00      0.00      0.00         0\n",
      "        dws       0.35      0.44      0.39     19547\n",
      "\n",
      "avg / total       0.77      0.71      0.72    165785\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okleinfeld/venv/ds_workshop/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "ff2_model = Sequential()\n",
    "ff2_model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))  # first hidden layer size is 32\n",
    "ff2_model.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu'))  # second hidden layer size is 32\n",
    "ff2_model.add(Dropout(dropout_rate))  # adding dropout layer\n",
    "ff2_model.add(Dense(6, activation='softmax'))  # applying softmax and cross entorpy loss\n",
    "ff2_model.compile(loss='categorical_crossentropy',optimizer='adam')\n",
    "ff2_model.fit(X_train, y_train_one_hot, batch_size=32, epochs=5)\n",
    "\n",
    "ff2_predictions = ff2_model.predict(X_test_real)\n",
    "ff2_test_predictions = np.array([np.argmax(prediction) for prediction in ff2_predictions])\n",
    "win_test_processor.evaluate_results(y_test_real, ff2_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intermediate neural conclusions:**\n",
    "* Normal feed forward does not perform much better than random forest\n",
    "* Adding another layer did not improve at all the F1-score\n",
    "* This might be because our sliding window is not an ideal input for time series neural network\n",
    "* Try a recurrent neural network such as LSTM instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Recurrent Neural Network\n",
    "* We would need to use raw data, and not summarized like Sliding Window\n",
    "* Results may differ between vector sizes of **raw history** encoding\n",
    "* Ideally we would predict each experiment separately but our data only contains dozens of experiments\n",
    "* This means that another discreteziation method is required\n",
    "* We will again use a **predefined vector size** (i.e. 10) in our raw history encoding\n",
    "* Thus, We do not expect significant improvement comparing to the Feed Forward models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawHistory:\n",
    "    \n",
    "    def __init__(self, origin_df, history_length, num_experiments, num_participants, exclude):\n",
    "        exps = [i for i in range(1,num_experiments + 1) if i != exclude]\n",
    "        parts = [i for i in range(1,num_participants + 1)]\n",
    "        smp_df = self.create_history_encoded_df(origin_df, history_length, expirements=exps, participants=parts)\n",
    "        self.history_length = history_length\n",
    "        self.df = smp_df\n",
    "\n",
    "    def create_history_encoded_single_exp(self, orig_df, history_length):\n",
    "        hist_df = orig_df.copy(deep=True) # later operations are \"in place\" so we need to avoid changing original dataframe\n",
    "        columns_to_shift = hist_df.columns[:-1] # omit the action column, we don't want to duplicate it\n",
    "        for i in range(1,history_length + 1):\n",
    "            shift_df = orig_df.shift(i)\n",
    "            for col_name in columns_to_shift:\n",
    "                new_col_name = \"prev_{0}_\".format(i) + col_name\n",
    "                hist_df[new_col_name] = shift_df[col_name] # add shifted column, aka history, as a column to orignal dataframe\n",
    "\n",
    "        hist_df = hist_df[history_length:] # we don't return the first \"history_length\" sample - they have missing history data\n",
    "        return hist_df\n",
    "\n",
    "    def create_history_encoded_df(self, orig_df, history_length, expirements, participants):\n",
    "        dfs_to_concate = []\n",
    "        cols_to_drop = ['partc', 'action_file_index']\n",
    "        for e in expirements:\n",
    "            for p in participants:\n",
    "                exp_df = orig_df[(orig_df['partc'] == p) & (orig_df['action_file_index'] == e)]\n",
    "                exp_df = exp_df.drop(cols_to_drop, axis=1)\n",
    "                exp_histoy_df = self.create_history_encoded_single_exp(exp_df, history_length)\n",
    "                dfs_to_concate.append(exp_histoy_df)\n",
    "        return pd.concat(dfs_to_concate, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variables for the raw history data frame creation\n",
    "num_experiments = 16\n",
    "num_participants = 24\n",
    "exclude = 10\n",
    "HISTORY_LEN = 10\n",
    "\n",
    "# create the raw history data frame for training\n",
    "train_hist_df = RawHistory(train_df, HISTORY_LEN, num_experiments, num_participants, exclude)\n",
    "train_hist_df = train_hist_df.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the raw history data frame for testing\n",
    "num_experiments = 18\n",
    "num_participants = 1\n",
    "exclude = 0\n",
    "HISTORY_LEN = 10\n",
    "\n",
    "test_hist_df = RawHistory(real_test_df, HISTORY_LEN, num_experiments, num_participants, exclude)\n",
    "test_hist_df = test_hist_df.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {'wlk': 0, 'sit': 1, \"std\": 2, \"ups\": 3, \"jog\": 4, \"dws\": 5}\n",
    "\n",
    "hist_train_processor = DataProcessingEval(train_hist_df, labels_dict=labels)\n",
    "X_train, y_train, _, _, _, _  = hist_train_processor.create_samples([1.0, 0, 0])\n",
    "\n",
    "hist_test_processor = DataProcessingEval(test_hist_df, labels_dict=labels)\n",
    "X_test_real, y_test_real, _, _, _, _ = hist_test_processor.create_samples([1.0, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The LSTM can deal with sequence of inputs, thus we don't need to explicitly encode our raw history as one long vectort as before.\n",
    "* We will pass to the LSTM model a sequence of 11 data points (10 history and actual data point)\n",
    "* Each data point is a vector of length 12 (our 12 origianl features)\n",
    "* Thus, we need to transform our raw encoding long vector to a sequence of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 12\n",
    "X_train = np.array(X_train)\n",
    "X_train = np.flip(X_train, axis=1)  # reverse the sequence from past to present\n",
    "X_train = X_train.reshape(-1, HISTORY_LEN + 1, NUM_FEATURES)  # reshape for LSTM sequence input (num_samples, 11, 12)\n",
    "\n",
    "X_test_real = np.array(X_test_real)\n",
    "X_test_real = np.flip(X_test_real, axis=1)\n",
    "X_test_real = X_test_real.reshape(-1, HISTORY_LEN + 1, NUM_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding\n",
    "\n",
    "dropout_rate = 0.5\n",
    "max_input_len,  data_point_dim = X_train.shape[1], X_train.shape[2]\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(32, return_sequences=False, dropout=dropout_rate, input_shape=(max_input_len, data_point_dim, )))\n",
    "lstm_model.add(Dense(6, activation='softmax'))  # applying softmax and cross entorpy loss\n",
    "lstm_model.compile(loss='categorical_crossentropy',optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1409265/1409265 [==============================] - 476s 338us/step - loss: 0.4948\n",
      "Epoch 2/5\n",
      "1409265/1409265 [==============================] - 475s 337us/step - loss: 0.3887\n",
      "Epoch 3/5\n",
      "1409265/1409265 [==============================] - 497s 353us/step - loss: 0.3646\n",
      "Epoch 4/5\n",
      "1409265/1409265 [==============================] - 503s 357us/step - loss: 0.3496\n",
      "Epoch 5/5\n",
      "1409265/1409265 [==============================] - 509s 361us/step - loss: 0.3409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11eabec88>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "num_activities = 6\n",
    "y_train_one_hot = np.array([to_categorical(t, num_activities) for t in y_train])\n",
    "\n",
    "# tranform y to one hot encoding vector of length 6 (we have 6 activities)\n",
    "lstm_model.fit(X_train, y_train_one_hot, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the LSTM model results on the real test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Printing classification report ----\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        wlk       0.81      0.48      0.60     52652\n",
      "        sit       0.48      0.97      0.64     35225\n",
      "        std       0.13      0.00      0.00     36561\n",
      "        ups       0.39      0.81      0.52     21800\n",
      "        jog       0.00      0.00      0.00         0\n",
      "        dws       0.43      0.38      0.40     19547\n",
      "\n",
      "avg / total       0.49      0.51      0.44    165785\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/okleinfeld/venv/ds_workshop/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "lstm_model_predictions = lstm_model.predict(X_test_real)\n",
    "lstm_model_test_predictions = np.argmax(lstm_model_predictions, axis=1)\n",
    "hist_test_processor.evaluate_results(y_test_real, lstm_model_test_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Applicative Predictions Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_MAIN_DIR = os.getcwd()\n",
    "path = os.path.join(PROJECT_MAIN_DIR, 'real-data')\n",
    "data_loader = NewDataLoader(path)\n",
    "real_test_df = data_loader.load_all_expirements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attitude.roll</th>\n",
       "      <th>attitude.pitch</th>\n",
       "      <th>attitude.yaw</th>\n",
       "      <th>gravity.x</th>\n",
       "      <th>gravity.y</th>\n",
       "      <th>gravity.z</th>\n",
       "      <th>rotationRate.x</th>\n",
       "      <th>rotationRate.y</th>\n",
       "      <th>rotationRate.z</th>\n",
       "      <th>userAcceleration.x</th>\n",
       "      <th>userAcceleration.y</th>\n",
       "      <th>userAcceleration.z</th>\n",
       "      <th>partc</th>\n",
       "      <th>action</th>\n",
       "      <th>action_file_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.167633</td>\n",
       "      <td>0.179202</td>\n",
       "      <td>0.030154</td>\n",
       "      <td>-0.164177</td>\n",
       "      <td>-0.178245</td>\n",
       "      <td>-0.970193</td>\n",
       "      <td>-0.613843</td>\n",
       "      <td>1.339701</td>\n",
       "      <td>0.479796</td>\n",
       "      <td>0.118248</td>\n",
       "      <td>0.071449</td>\n",
       "      <td>-0.070273</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.154363</td>\n",
       "      <td>0.171377</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>-0.151499</td>\n",
       "      <td>-0.170540</td>\n",
       "      <td>-0.973635</td>\n",
       "      <td>-0.738818</td>\n",
       "      <td>1.409082</td>\n",
       "      <td>0.606134</td>\n",
       "      <td>0.117502</td>\n",
       "      <td>0.042686</td>\n",
       "      <td>0.038011</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.141519</td>\n",
       "      <td>0.160889</td>\n",
       "      <td>0.039637</td>\n",
       "      <td>-0.139226</td>\n",
       "      <td>-0.160196</td>\n",
       "      <td>-0.977217</td>\n",
       "      <td>-1.048560</td>\n",
       "      <td>1.327002</td>\n",
       "      <td>0.602302</td>\n",
       "      <td>0.103520</td>\n",
       "      <td>0.058405</td>\n",
       "      <td>0.177809</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.130109</td>\n",
       "      <td>0.146934</td>\n",
       "      <td>0.043551</td>\n",
       "      <td>-0.128345</td>\n",
       "      <td>-0.146405</td>\n",
       "      <td>-0.980863</td>\n",
       "      <td>-1.415643</td>\n",
       "      <td>1.152735</td>\n",
       "      <td>0.554232</td>\n",
       "      <td>0.072650</td>\n",
       "      <td>0.022306</td>\n",
       "      <td>0.264677</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.120020</td>\n",
       "      <td>0.128703</td>\n",
       "      <td>0.046674</td>\n",
       "      <td>-0.118741</td>\n",
       "      <td>-0.128348</td>\n",
       "      <td>-0.984595</td>\n",
       "      <td>-1.875311</td>\n",
       "      <td>1.009978</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>-0.014910</td>\n",
       "      <td>-0.006723</td>\n",
       "      <td>0.293112</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.110287</td>\n",
       "      <td>0.106239</td>\n",
       "      <td>0.048990</td>\n",
       "      <td>-0.109443</td>\n",
       "      <td>-0.106040</td>\n",
       "      <td>-0.988321</td>\n",
       "      <td>-2.280296</td>\n",
       "      <td>1.012389</td>\n",
       "      <td>0.464374</td>\n",
       "      <td>-0.050790</td>\n",
       "      <td>0.031104</td>\n",
       "      <td>0.224405</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.097879</td>\n",
       "      <td>0.081424</td>\n",
       "      <td>0.050295</td>\n",
       "      <td>-0.097399</td>\n",
       "      <td>-0.081334</td>\n",
       "      <td>-0.991916</td>\n",
       "      <td>-2.466738</td>\n",
       "      <td>1.343851</td>\n",
       "      <td>0.356050</td>\n",
       "      <td>-0.032926</td>\n",
       "      <td>0.112645</td>\n",
       "      <td>0.153538</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.081616</td>\n",
       "      <td>0.056942</td>\n",
       "      <td>0.051152</td>\n",
       "      <td>-0.081394</td>\n",
       "      <td>-0.056911</td>\n",
       "      <td>-0.995056</td>\n",
       "      <td>-2.372062</td>\n",
       "      <td>1.697859</td>\n",
       "      <td>0.293400</td>\n",
       "      <td>0.027988</td>\n",
       "      <td>0.159221</td>\n",
       "      <td>0.006546</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.063591</td>\n",
       "      <td>0.035518</td>\n",
       "      <td>0.052560</td>\n",
       "      <td>-0.063509</td>\n",
       "      <td>-0.035510</td>\n",
       "      <td>-0.997349</td>\n",
       "      <td>-2.003520</td>\n",
       "      <td>1.807552</td>\n",
       "      <td>0.296102</td>\n",
       "      <td>0.098329</td>\n",
       "      <td>0.103732</td>\n",
       "      <td>-0.231075</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.045582</td>\n",
       "      <td>0.019461</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>-0.045557</td>\n",
       "      <td>-0.019460</td>\n",
       "      <td>-0.998772</td>\n",
       "      <td>-1.429766</td>\n",
       "      <td>1.786230</td>\n",
       "      <td>0.263532</td>\n",
       "      <td>0.114100</td>\n",
       "      <td>0.047566</td>\n",
       "      <td>-0.373146</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.027947</td>\n",
       "      <td>0.009487</td>\n",
       "      <td>0.055880</td>\n",
       "      <td>-0.027942</td>\n",
       "      <td>-0.009487</td>\n",
       "      <td>-0.999565</td>\n",
       "      <td>-0.847974</td>\n",
       "      <td>1.746633</td>\n",
       "      <td>0.143763</td>\n",
       "      <td>0.026935</td>\n",
       "      <td>0.062557</td>\n",
       "      <td>-0.373894</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.009443</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.056389</td>\n",
       "      <td>-0.009442</td>\n",
       "      <td>-0.003673</td>\n",
       "      <td>-0.999949</td>\n",
       "      <td>-0.510170</td>\n",
       "      <td>1.887414</td>\n",
       "      <td>0.042289</td>\n",
       "      <td>-0.046481</td>\n",
       "      <td>0.134258</td>\n",
       "      <td>-0.298300</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.012090</td>\n",
       "      <td>-0.000648</td>\n",
       "      <td>0.057225</td>\n",
       "      <td>0.012090</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>-0.999927</td>\n",
       "      <td>-0.417591</td>\n",
       "      <td>2.238648</td>\n",
       "      <td>0.116242</td>\n",
       "      <td>0.041178</td>\n",
       "      <td>0.171593</td>\n",
       "      <td>-0.161313</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.038383</td>\n",
       "      <td>-0.005440</td>\n",
       "      <td>0.060698</td>\n",
       "      <td>0.038373</td>\n",
       "      <td>0.005440</td>\n",
       "      <td>-0.999249</td>\n",
       "      <td>-0.526470</td>\n",
       "      <td>2.742056</td>\n",
       "      <td>0.418061</td>\n",
       "      <td>0.174746</td>\n",
       "      <td>0.099403</td>\n",
       "      <td>-0.027149</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.070169</td>\n",
       "      <td>-0.012867</td>\n",
       "      <td>0.067953</td>\n",
       "      <td>0.070106</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>-0.997457</td>\n",
       "      <td>-0.874100</td>\n",
       "      <td>3.277574</td>\n",
       "      <td>0.758167</td>\n",
       "      <td>0.172982</td>\n",
       "      <td>0.095715</td>\n",
       "      <td>0.097081</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.105771</td>\n",
       "      <td>-0.023940</td>\n",
       "      <td>0.078408</td>\n",
       "      <td>0.105544</td>\n",
       "      <td>0.023938</td>\n",
       "      <td>-0.994126</td>\n",
       "      <td>-1.285745</td>\n",
       "      <td>3.558701</td>\n",
       "      <td>0.985813</td>\n",
       "      <td>0.122255</td>\n",
       "      <td>0.145908</td>\n",
       "      <td>0.169496</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.140721</td>\n",
       "      <td>-0.037238</td>\n",
       "      <td>0.090587</td>\n",
       "      <td>0.140159</td>\n",
       "      <td>0.037229</td>\n",
       "      <td>-0.989429</td>\n",
       "      <td>-1.494961</td>\n",
       "      <td>3.351523</td>\n",
       "      <td>1.037321</td>\n",
       "      <td>0.072518</td>\n",
       "      <td>0.077136</td>\n",
       "      <td>0.104327</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.172000</td>\n",
       "      <td>-0.050623</td>\n",
       "      <td>0.102265</td>\n",
       "      <td>0.170934</td>\n",
       "      <td>0.050601</td>\n",
       "      <td>-0.983982</td>\n",
       "      <td>-1.478734</td>\n",
       "      <td>2.958095</td>\n",
       "      <td>0.887700</td>\n",
       "      <td>-0.034353</td>\n",
       "      <td>0.019040</td>\n",
       "      <td>-0.025478</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.200236</td>\n",
       "      <td>-0.063015</td>\n",
       "      <td>0.111752</td>\n",
       "      <td>0.198506</td>\n",
       "      <td>0.062973</td>\n",
       "      <td>-0.978074</td>\n",
       "      <td>-1.345200</td>\n",
       "      <td>2.692793</td>\n",
       "      <td>0.628468</td>\n",
       "      <td>-0.094807</td>\n",
       "      <td>0.024185</td>\n",
       "      <td>-0.090437</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.227011</td>\n",
       "      <td>-0.074057</td>\n",
       "      <td>0.118688</td>\n",
       "      <td>0.224450</td>\n",
       "      <td>0.073989</td>\n",
       "      <td>-0.971673</td>\n",
       "      <td>-1.175140</td>\n",
       "      <td>2.594412</td>\n",
       "      <td>0.379318</td>\n",
       "      <td>-0.103218</td>\n",
       "      <td>0.019623</td>\n",
       "      <td>-0.104743</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.253180</td>\n",
       "      <td>-0.084042</td>\n",
       "      <td>0.123363</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.083943</td>\n",
       "      <td>-0.964704</td>\n",
       "      <td>-1.040009</td>\n",
       "      <td>2.550704</td>\n",
       "      <td>0.158374</td>\n",
       "      <td>-0.080853</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>-0.095309</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.276430</td>\n",
       "      <td>-0.092642</td>\n",
       "      <td>0.125740</td>\n",
       "      <td>0.271753</td>\n",
       "      <td>0.092510</td>\n",
       "      <td>-0.957910</td>\n",
       "      <td>-0.814316</td>\n",
       "      <td>2.131744</td>\n",
       "      <td>-0.046658</td>\n",
       "      <td>-0.054467</td>\n",
       "      <td>-0.012035</td>\n",
       "      <td>-0.106802</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.290492</td>\n",
       "      <td>-0.098822</td>\n",
       "      <td>0.125591</td>\n",
       "      <td>0.285026</td>\n",
       "      <td>0.098661</td>\n",
       "      <td>-0.953429</td>\n",
       "      <td>-0.533611</td>\n",
       "      <td>1.162883</td>\n",
       "      <td>-0.235130</td>\n",
       "      <td>0.050042</td>\n",
       "      <td>-0.016996</td>\n",
       "      <td>-0.072191</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.300650</td>\n",
       "      <td>-0.106718</td>\n",
       "      <td>0.124712</td>\n",
       "      <td>0.294456</td>\n",
       "      <td>0.106516</td>\n",
       "      <td>-0.949710</td>\n",
       "      <td>-0.843820</td>\n",
       "      <td>1.058056</td>\n",
       "      <td>-0.312171</td>\n",
       "      <td>0.087532</td>\n",
       "      <td>0.080649</td>\n",
       "      <td>0.162891</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.314913</td>\n",
       "      <td>-0.119627</td>\n",
       "      <td>0.127719</td>\n",
       "      <td>0.307521</td>\n",
       "      <td>0.119342</td>\n",
       "      <td>-0.944028</td>\n",
       "      <td>-1.460950</td>\n",
       "      <td>1.488719</td>\n",
       "      <td>-0.015854</td>\n",
       "      <td>0.173909</td>\n",
       "      <td>0.159802</td>\n",
       "      <td>0.387647</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.331861</td>\n",
       "      <td>-0.135441</td>\n",
       "      <td>0.135261</td>\n",
       "      <td>0.322819</td>\n",
       "      <td>0.135028</td>\n",
       "      <td>-0.936779</td>\n",
       "      <td>-1.776160</td>\n",
       "      <td>1.582192</td>\n",
       "      <td>0.260505</td>\n",
       "      <td>0.265682</td>\n",
       "      <td>0.171140</td>\n",
       "      <td>0.282955</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.347712</td>\n",
       "      <td>-0.150911</td>\n",
       "      <td>0.144773</td>\n",
       "      <td>0.336875</td>\n",
       "      <td>0.150339</td>\n",
       "      <td>-0.929469</td>\n",
       "      <td>-1.735047</td>\n",
       "      <td>1.378622</td>\n",
       "      <td>0.402879</td>\n",
       "      <td>0.208749</td>\n",
       "      <td>0.160681</td>\n",
       "      <td>0.065868</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.361408</td>\n",
       "      <td>-0.164371</td>\n",
       "      <td>0.155491</td>\n",
       "      <td>0.348826</td>\n",
       "      <td>0.163632</td>\n",
       "      <td>-0.922792</td>\n",
       "      <td>-1.575321</td>\n",
       "      <td>1.134703</td>\n",
       "      <td>0.559924</td>\n",
       "      <td>0.151388</td>\n",
       "      <td>0.104343</td>\n",
       "      <td>-0.047408</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.373481</td>\n",
       "      <td>-0.175360</td>\n",
       "      <td>0.167032</td>\n",
       "      <td>0.359264</td>\n",
       "      <td>0.174462</td>\n",
       "      <td>-0.916784</td>\n",
       "      <td>-1.371356</td>\n",
       "      <td>0.967859</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.097157</td>\n",
       "      <td>0.076102</td>\n",
       "      <td>-0.095545</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.384833</td>\n",
       "      <td>-0.183855</td>\n",
       "      <td>0.178830</td>\n",
       "      <td>0.369078</td>\n",
       "      <td>0.182821</td>\n",
       "      <td>-0.911240</td>\n",
       "      <td>-1.158767</td>\n",
       "      <td>0.921977</td>\n",
       "      <td>0.778858</td>\n",
       "      <td>0.063067</td>\n",
       "      <td>0.076929</td>\n",
       "      <td>-0.122467</td>\n",
       "      <td>1</td>\n",
       "      <td>ups</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13414</th>\n",
       "      <td>-0.217224</td>\n",
       "      <td>0.373665</td>\n",
       "      <td>0.172216</td>\n",
       "      <td>-0.200648</td>\n",
       "      <td>-0.365030</td>\n",
       "      <td>-0.909117</td>\n",
       "      <td>0.690101</td>\n",
       "      <td>-0.151291</td>\n",
       "      <td>0.619265</td>\n",
       "      <td>-0.003896</td>\n",
       "      <td>0.081582</td>\n",
       "      <td>0.014021</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13415</th>\n",
       "      <td>-0.222659</td>\n",
       "      <td>0.378429</td>\n",
       "      <td>0.180296</td>\n",
       "      <td>-0.205199</td>\n",
       "      <td>-0.369461</td>\n",
       "      <td>-0.906306</td>\n",
       "      <td>0.605125</td>\n",
       "      <td>-0.274486</td>\n",
       "      <td>0.621253</td>\n",
       "      <td>-0.058976</td>\n",
       "      <td>0.088989</td>\n",
       "      <td>0.062938</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13416</th>\n",
       "      <td>-0.229231</td>\n",
       "      <td>0.382305</td>\n",
       "      <td>0.187872</td>\n",
       "      <td>-0.210824</td>\n",
       "      <td>-0.373060</td>\n",
       "      <td>-0.903537</td>\n",
       "      <td>0.513114</td>\n",
       "      <td>-0.400823</td>\n",
       "      <td>0.582373</td>\n",
       "      <td>-0.091025</td>\n",
       "      <td>0.082106</td>\n",
       "      <td>0.081195</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13417</th>\n",
       "      <td>-0.236347</td>\n",
       "      <td>0.385475</td>\n",
       "      <td>0.194769</td>\n",
       "      <td>-0.216971</td>\n",
       "      <td>-0.375999</td>\n",
       "      <td>-0.900859</td>\n",
       "      <td>0.435841</td>\n",
       "      <td>-0.459003</td>\n",
       "      <td>0.530750</td>\n",
       "      <td>-0.091715</td>\n",
       "      <td>0.079567</td>\n",
       "      <td>0.081615</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13418</th>\n",
       "      <td>-0.243442</td>\n",
       "      <td>0.388057</td>\n",
       "      <td>0.201095</td>\n",
       "      <td>-0.223122</td>\n",
       "      <td>-0.378390</td>\n",
       "      <td>-0.898352</td>\n",
       "      <td>0.377287</td>\n",
       "      <td>-0.468907</td>\n",
       "      <td>0.497510</td>\n",
       "      <td>-0.069695</td>\n",
       "      <td>0.083987</td>\n",
       "      <td>0.076117</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13419</th>\n",
       "      <td>-0.250710</td>\n",
       "      <td>0.390202</td>\n",
       "      <td>0.207247</td>\n",
       "      <td>-0.229443</td>\n",
       "      <td>-0.380376</td>\n",
       "      <td>-0.895918</td>\n",
       "      <td>0.339516</td>\n",
       "      <td>-0.503038</td>\n",
       "      <td>0.496422</td>\n",
       "      <td>-0.031910</td>\n",
       "      <td>0.074650</td>\n",
       "      <td>0.054961</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13420</th>\n",
       "      <td>-0.258547</td>\n",
       "      <td>0.392000</td>\n",
       "      <td>0.213490</td>\n",
       "      <td>-0.236282</td>\n",
       "      <td>-0.382037</td>\n",
       "      <td>-0.893430</td>\n",
       "      <td>0.318448</td>\n",
       "      <td>-0.551457</td>\n",
       "      <td>0.510312</td>\n",
       "      <td>-0.001541</td>\n",
       "      <td>0.061069</td>\n",
       "      <td>0.032087</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13421</th>\n",
       "      <td>-0.265091</td>\n",
       "      <td>0.393898</td>\n",
       "      <td>0.219075</td>\n",
       "      <td>-0.241933</td>\n",
       "      <td>-0.383790</td>\n",
       "      <td>-0.891164</td>\n",
       "      <td>0.321122</td>\n",
       "      <td>-0.367714</td>\n",
       "      <td>0.412957</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>0.064668</td>\n",
       "      <td>0.020818</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13422</th>\n",
       "      <td>-0.266615</td>\n",
       "      <td>0.396675</td>\n",
       "      <td>0.222315</td>\n",
       "      <td>-0.243009</td>\n",
       "      <td>-0.386354</td>\n",
       "      <td>-0.889762</td>\n",
       "      <td>0.358593</td>\n",
       "      <td>0.083670</td>\n",
       "      <td>0.128063</td>\n",
       "      <td>-0.321139</td>\n",
       "      <td>0.082780</td>\n",
       "      <td>-0.008615</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13423</th>\n",
       "      <td>-0.263565</td>\n",
       "      <td>0.400540</td>\n",
       "      <td>0.221774</td>\n",
       "      <td>-0.239903</td>\n",
       "      <td>-0.389916</td>\n",
       "      <td>-0.889051</td>\n",
       "      <td>0.348540</td>\n",
       "      <td>0.323373</td>\n",
       "      <td>-0.233709</td>\n",
       "      <td>-0.466380</td>\n",
       "      <td>0.060509</td>\n",
       "      <td>-0.048938</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13424</th>\n",
       "      <td>-0.258490</td>\n",
       "      <td>0.404285</td>\n",
       "      <td>0.218517</td>\n",
       "      <td>-0.235014</td>\n",
       "      <td>-0.393362</td>\n",
       "      <td>-0.888839</td>\n",
       "      <td>0.257428</td>\n",
       "      <td>0.394132</td>\n",
       "      <td>-0.414003</td>\n",
       "      <td>-0.403215</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>0.022261</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13425</th>\n",
       "      <td>-0.252794</td>\n",
       "      <td>0.407140</td>\n",
       "      <td>0.214780</td>\n",
       "      <td>-0.229665</td>\n",
       "      <td>-0.395985</td>\n",
       "      <td>-0.889072</td>\n",
       "      <td>0.170013</td>\n",
       "      <td>0.415726</td>\n",
       "      <td>-0.385773</td>\n",
       "      <td>-0.263209</td>\n",
       "      <td>0.028507</td>\n",
       "      <td>0.078281</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13426</th>\n",
       "      <td>-0.248679</td>\n",
       "      <td>0.409433</td>\n",
       "      <td>0.212167</td>\n",
       "      <td>-0.225781</td>\n",
       "      <td>-0.398090</td>\n",
       "      <td>-0.889127</td>\n",
       "      <td>0.167001</td>\n",
       "      <td>0.257060</td>\n",
       "      <td>-0.247470</td>\n",
       "      <td>-0.125599</td>\n",
       "      <td>0.028918</td>\n",
       "      <td>0.025342</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13427</th>\n",
       "      <td>-0.247806</td>\n",
       "      <td>0.411966</td>\n",
       "      <td>0.211816</td>\n",
       "      <td>-0.224757</td>\n",
       "      <td>-0.400411</td>\n",
       "      <td>-0.888344</td>\n",
       "      <td>0.267501</td>\n",
       "      <td>0.014950</td>\n",
       "      <td>-0.037578</td>\n",
       "      <td>-0.003423</td>\n",
       "      <td>-0.027354</td>\n",
       "      <td>-0.051033</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13428</th>\n",
       "      <td>-0.249825</td>\n",
       "      <td>0.415486</td>\n",
       "      <td>0.214190</td>\n",
       "      <td>-0.226199</td>\n",
       "      <td>-0.403635</td>\n",
       "      <td>-0.886517</td>\n",
       "      <td>0.435141</td>\n",
       "      <td>-0.141346</td>\n",
       "      <td>0.172410</td>\n",
       "      <td>0.048053</td>\n",
       "      <td>-0.062155</td>\n",
       "      <td>-0.092212</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13429</th>\n",
       "      <td>-0.254159</td>\n",
       "      <td>0.420014</td>\n",
       "      <td>0.218810</td>\n",
       "      <td>-0.229578</td>\n",
       "      <td>-0.407773</td>\n",
       "      <td>-0.883750</td>\n",
       "      <td>0.568520</td>\n",
       "      <td>-0.276652</td>\n",
       "      <td>0.327679</td>\n",
       "      <td>0.025232</td>\n",
       "      <td>-0.044863</td>\n",
       "      <td>-0.086541</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13430</th>\n",
       "      <td>-0.260474</td>\n",
       "      <td>0.424995</td>\n",
       "      <td>0.224807</td>\n",
       "      <td>-0.234628</td>\n",
       "      <td>-0.412316</td>\n",
       "      <td>-0.880309</td>\n",
       "      <td>0.625730</td>\n",
       "      <td>-0.415188</td>\n",
       "      <td>0.417500</td>\n",
       "      <td>-0.022208</td>\n",
       "      <td>-0.011313</td>\n",
       "      <td>-0.043779</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13431</th>\n",
       "      <td>-0.268049</td>\n",
       "      <td>0.429833</td>\n",
       "      <td>0.231123</td>\n",
       "      <td>-0.240758</td>\n",
       "      <td>-0.416719</td>\n",
       "      <td>-0.876573</td>\n",
       "      <td>0.603726</td>\n",
       "      <td>-0.512972</td>\n",
       "      <td>0.422644</td>\n",
       "      <td>-0.063868</td>\n",
       "      <td>0.016420</td>\n",
       "      <td>0.012925</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13432</th>\n",
       "      <td>-0.276098</td>\n",
       "      <td>0.434149</td>\n",
       "      <td>0.237067</td>\n",
       "      <td>-0.247314</td>\n",
       "      <td>-0.420638</td>\n",
       "      <td>-0.872868</td>\n",
       "      <td>0.535731</td>\n",
       "      <td>-0.561337</td>\n",
       "      <td>0.392541</td>\n",
       "      <td>-0.071381</td>\n",
       "      <td>0.039611</td>\n",
       "      <td>0.052266</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13433</th>\n",
       "      <td>-0.283915</td>\n",
       "      <td>0.437678</td>\n",
       "      <td>0.242435</td>\n",
       "      <td>-0.253712</td>\n",
       "      <td>-0.423838</td>\n",
       "      <td>-0.869477</td>\n",
       "      <td>0.448781</td>\n",
       "      <td>-0.542783</td>\n",
       "      <td>0.360524</td>\n",
       "      <td>-0.049908</td>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.071275</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13434</th>\n",
       "      <td>-0.291505</td>\n",
       "      <td>0.440460</td>\n",
       "      <td>0.247349</td>\n",
       "      <td>-0.259964</td>\n",
       "      <td>-0.426356</td>\n",
       "      <td>-0.866394</td>\n",
       "      <td>0.375423</td>\n",
       "      <td>-0.552782</td>\n",
       "      <td>0.340226</td>\n",
       "      <td>-0.025528</td>\n",
       "      <td>0.050379</td>\n",
       "      <td>0.064133</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13435</th>\n",
       "      <td>-0.299397</td>\n",
       "      <td>0.442834</td>\n",
       "      <td>0.251791</td>\n",
       "      <td>-0.266494</td>\n",
       "      <td>-0.428502</td>\n",
       "      <td>-0.863346</td>\n",
       "      <td>0.330595</td>\n",
       "      <td>-0.602148</td>\n",
       "      <td>0.302538</td>\n",
       "      <td>-0.009903</td>\n",
       "      <td>0.044499</td>\n",
       "      <td>0.032856</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13436</th>\n",
       "      <td>-0.306774</td>\n",
       "      <td>0.444951</td>\n",
       "      <td>0.255810</td>\n",
       "      <td>-0.272581</td>\n",
       "      <td>-0.430414</td>\n",
       "      <td>-0.860490</td>\n",
       "      <td>0.299249</td>\n",
       "      <td>-0.541737</td>\n",
       "      <td>0.273743</td>\n",
       "      <td>0.009214</td>\n",
       "      <td>0.055047</td>\n",
       "      <td>0.028275</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13437</th>\n",
       "      <td>-0.313139</td>\n",
       "      <td>0.446862</td>\n",
       "      <td>0.259518</td>\n",
       "      <td>-0.277798</td>\n",
       "      <td>-0.432138</td>\n",
       "      <td>-0.857953</td>\n",
       "      <td>0.277268</td>\n",
       "      <td>-0.450595</td>\n",
       "      <td>0.252545</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.054727</td>\n",
       "      <td>0.029523</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13438</th>\n",
       "      <td>-0.318507</td>\n",
       "      <td>0.448701</td>\n",
       "      <td>0.262802</td>\n",
       "      <td>-0.282150</td>\n",
       "      <td>-0.433796</td>\n",
       "      <td>-0.855693</td>\n",
       "      <td>0.262801</td>\n",
       "      <td>-0.372523</td>\n",
       "      <td>0.211931</td>\n",
       "      <td>-0.005585</td>\n",
       "      <td>0.047703</td>\n",
       "      <td>0.027248</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13439</th>\n",
       "      <td>-0.323071</td>\n",
       "      <td>0.450540</td>\n",
       "      <td>0.265650</td>\n",
       "      <td>-0.285799</td>\n",
       "      <td>-0.435452</td>\n",
       "      <td>-0.853639</td>\n",
       "      <td>0.249375</td>\n",
       "      <td>-0.318617</td>\n",
       "      <td>0.175547</td>\n",
       "      <td>-0.017820</td>\n",
       "      <td>0.041790</td>\n",
       "      <td>0.034105</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13440</th>\n",
       "      <td>-0.327241</td>\n",
       "      <td>0.452226</td>\n",
       "      <td>0.268226</td>\n",
       "      <td>-0.289120</td>\n",
       "      <td>-0.436969</td>\n",
       "      <td>-0.851743</td>\n",
       "      <td>0.227398</td>\n",
       "      <td>-0.298865</td>\n",
       "      <td>0.162747</td>\n",
       "      <td>-0.021793</td>\n",
       "      <td>0.035129</td>\n",
       "      <td>0.037595</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13441</th>\n",
       "      <td>-0.331523</td>\n",
       "      <td>0.453563</td>\n",
       "      <td>0.270766</td>\n",
       "      <td>-0.292574</td>\n",
       "      <td>-0.438171</td>\n",
       "      <td>-0.849944</td>\n",
       "      <td>0.194780</td>\n",
       "      <td>-0.325347</td>\n",
       "      <td>0.173501</td>\n",
       "      <td>-0.026456</td>\n",
       "      <td>0.026596</td>\n",
       "      <td>0.034865</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13442</th>\n",
       "      <td>-0.336480</td>\n",
       "      <td>0.454719</td>\n",
       "      <td>0.273490</td>\n",
       "      <td>-0.296616</td>\n",
       "      <td>-0.439210</td>\n",
       "      <td>-0.848005</td>\n",
       "      <td>0.193589</td>\n",
       "      <td>-0.393524</td>\n",
       "      <td>0.200196</td>\n",
       "      <td>-0.030990</td>\n",
       "      <td>0.015427</td>\n",
       "      <td>0.008832</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13443</th>\n",
       "      <td>-0.342265</td>\n",
       "      <td>0.455180</td>\n",
       "      <td>0.276378</td>\n",
       "      <td>-0.301449</td>\n",
       "      <td>-0.439624</td>\n",
       "      <td>-0.846084</td>\n",
       "      <td>0.082093</td>\n",
       "      <td>-0.468608</td>\n",
       "      <td>0.230351</td>\n",
       "      <td>-0.034977</td>\n",
       "      <td>0.010196</td>\n",
       "      <td>0.069900</td>\n",
       "      <td>1</td>\n",
       "      <td>std</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165965 rows  15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       attitude.roll  attitude.pitch  attitude.yaw  gravity.x  gravity.y  \\\n",
       "0          -0.167633        0.179202      0.030154  -0.164177  -0.178245   \n",
       "1          -0.154363        0.171377      0.034900  -0.151499  -0.170540   \n",
       "2          -0.141519        0.160889      0.039637  -0.139226  -0.160196   \n",
       "3          -0.130109        0.146934      0.043551  -0.128345  -0.146405   \n",
       "4          -0.120020        0.128703      0.046674  -0.118741  -0.128348   \n",
       "5          -0.110287        0.106239      0.048990  -0.109443  -0.106040   \n",
       "6          -0.097879        0.081424      0.050295  -0.097399  -0.081334   \n",
       "7          -0.081616        0.056942      0.051152  -0.081394  -0.056911   \n",
       "8          -0.063591        0.035518      0.052560  -0.063509  -0.035510   \n",
       "9          -0.045582        0.019461      0.054475  -0.045557  -0.019460   \n",
       "10         -0.027947        0.009487      0.055880  -0.027942  -0.009487   \n",
       "11         -0.009443        0.003673      0.056389  -0.009442  -0.003673   \n",
       "12          0.012090       -0.000648      0.057225   0.012090   0.000648   \n",
       "13          0.038383       -0.005440      0.060698   0.038373   0.005440   \n",
       "14          0.070169       -0.012867      0.067953   0.070106   0.012867   \n",
       "15          0.105771       -0.023940      0.078408   0.105544   0.023938   \n",
       "16          0.140721       -0.037238      0.090587   0.140159   0.037229   \n",
       "17          0.172000       -0.050623      0.102265   0.170934   0.050601   \n",
       "18          0.200236       -0.063015      0.111752   0.198506   0.062973   \n",
       "19          0.227011       -0.074057      0.118688   0.224450   0.073989   \n",
       "20          0.253180       -0.084042      0.123363   0.249600   0.083943   \n",
       "21          0.276430       -0.092642      0.125740   0.271753   0.092510   \n",
       "22          0.290492       -0.098822      0.125591   0.285026   0.098661   \n",
       "23          0.300650       -0.106718      0.124712   0.294456   0.106516   \n",
       "24          0.314913       -0.119627      0.127719   0.307521   0.119342   \n",
       "25          0.331861       -0.135441      0.135261   0.322819   0.135028   \n",
       "26          0.347712       -0.150911      0.144773   0.336875   0.150339   \n",
       "27          0.361408       -0.164371      0.155491   0.348826   0.163632   \n",
       "28          0.373481       -0.175360      0.167032   0.359264   0.174462   \n",
       "29          0.384833       -0.183855      0.178830   0.369078   0.182821   \n",
       "...              ...             ...           ...        ...        ...   \n",
       "13414      -0.217224        0.373665      0.172216  -0.200648  -0.365030   \n",
       "13415      -0.222659        0.378429      0.180296  -0.205199  -0.369461   \n",
       "13416      -0.229231        0.382305      0.187872  -0.210824  -0.373060   \n",
       "13417      -0.236347        0.385475      0.194769  -0.216971  -0.375999   \n",
       "13418      -0.243442        0.388057      0.201095  -0.223122  -0.378390   \n",
       "13419      -0.250710        0.390202      0.207247  -0.229443  -0.380376   \n",
       "13420      -0.258547        0.392000      0.213490  -0.236282  -0.382037   \n",
       "13421      -0.265091        0.393898      0.219075  -0.241933  -0.383790   \n",
       "13422      -0.266615        0.396675      0.222315  -0.243009  -0.386354   \n",
       "13423      -0.263565        0.400540      0.221774  -0.239903  -0.389916   \n",
       "13424      -0.258490        0.404285      0.218517  -0.235014  -0.393362   \n",
       "13425      -0.252794        0.407140      0.214780  -0.229665  -0.395985   \n",
       "13426      -0.248679        0.409433      0.212167  -0.225781  -0.398090   \n",
       "13427      -0.247806        0.411966      0.211816  -0.224757  -0.400411   \n",
       "13428      -0.249825        0.415486      0.214190  -0.226199  -0.403635   \n",
       "13429      -0.254159        0.420014      0.218810  -0.229578  -0.407773   \n",
       "13430      -0.260474        0.424995      0.224807  -0.234628  -0.412316   \n",
       "13431      -0.268049        0.429833      0.231123  -0.240758  -0.416719   \n",
       "13432      -0.276098        0.434149      0.237067  -0.247314  -0.420638   \n",
       "13433      -0.283915        0.437678      0.242435  -0.253712  -0.423838   \n",
       "13434      -0.291505        0.440460      0.247349  -0.259964  -0.426356   \n",
       "13435      -0.299397        0.442834      0.251791  -0.266494  -0.428502   \n",
       "13436      -0.306774        0.444951      0.255810  -0.272581  -0.430414   \n",
       "13437      -0.313139        0.446862      0.259518  -0.277798  -0.432138   \n",
       "13438      -0.318507        0.448701      0.262802  -0.282150  -0.433796   \n",
       "13439      -0.323071        0.450540      0.265650  -0.285799  -0.435452   \n",
       "13440      -0.327241        0.452226      0.268226  -0.289120  -0.436969   \n",
       "13441      -0.331523        0.453563      0.270766  -0.292574  -0.438171   \n",
       "13442      -0.336480        0.454719      0.273490  -0.296616  -0.439210   \n",
       "13443      -0.342265        0.455180      0.276378  -0.301449  -0.439624   \n",
       "\n",
       "       gravity.z  rotationRate.x  rotationRate.y  rotationRate.z  \\\n",
       "0      -0.970193       -0.613843        1.339701        0.479796   \n",
       "1      -0.973635       -0.738818        1.409082        0.606134   \n",
       "2      -0.977217       -1.048560        1.327002        0.602302   \n",
       "3      -0.980863       -1.415643        1.152735        0.554232   \n",
       "4      -0.984595       -1.875311        1.009978        0.522600   \n",
       "5      -0.988321       -2.280296        1.012389        0.464374   \n",
       "6      -0.991916       -2.466738        1.343851        0.356050   \n",
       "7      -0.995056       -2.372062        1.697859        0.293400   \n",
       "8      -0.997349       -2.003520        1.807552        0.296102   \n",
       "9      -0.998772       -1.429766        1.786230        0.263532   \n",
       "10     -0.999565       -0.847974        1.746633        0.143763   \n",
       "11     -0.999949       -0.510170        1.887414        0.042289   \n",
       "12     -0.999927       -0.417591        2.238648        0.116242   \n",
       "13     -0.999249       -0.526470        2.742056        0.418061   \n",
       "14     -0.997457       -0.874100        3.277574        0.758167   \n",
       "15     -0.994126       -1.285745        3.558701        0.985813   \n",
       "16     -0.989429       -1.494961        3.351523        1.037321   \n",
       "17     -0.983982       -1.478734        2.958095        0.887700   \n",
       "18     -0.978074       -1.345200        2.692793        0.628468   \n",
       "19     -0.971673       -1.175140        2.594412        0.379318   \n",
       "20     -0.964704       -1.040009        2.550704        0.158374   \n",
       "21     -0.957910       -0.814316        2.131744       -0.046658   \n",
       "22     -0.953429       -0.533611        1.162883       -0.235130   \n",
       "23     -0.949710       -0.843820        1.058056       -0.312171   \n",
       "24     -0.944028       -1.460950        1.488719       -0.015854   \n",
       "25     -0.936779       -1.776160        1.582192        0.260505   \n",
       "26     -0.929469       -1.735047        1.378622        0.402879   \n",
       "27     -0.922792       -1.575321        1.134703        0.559924   \n",
       "28     -0.916784       -1.371356        0.967859        0.691275   \n",
       "29     -0.911240       -1.158767        0.921977        0.778858   \n",
       "...          ...             ...             ...             ...   \n",
       "13414  -0.909117        0.690101       -0.151291        0.619265   \n",
       "13415  -0.906306        0.605125       -0.274486        0.621253   \n",
       "13416  -0.903537        0.513114       -0.400823        0.582373   \n",
       "13417  -0.900859        0.435841       -0.459003        0.530750   \n",
       "13418  -0.898352        0.377287       -0.468907        0.497510   \n",
       "13419  -0.895918        0.339516       -0.503038        0.496422   \n",
       "13420  -0.893430        0.318448       -0.551457        0.510312   \n",
       "13421  -0.891164        0.321122       -0.367714        0.412957   \n",
       "13422  -0.889762        0.358593        0.083670        0.128063   \n",
       "13423  -0.889051        0.348540        0.323373       -0.233709   \n",
       "13424  -0.888839        0.257428        0.394132       -0.414003   \n",
       "13425  -0.889072        0.170013        0.415726       -0.385773   \n",
       "13426  -0.889127        0.167001        0.257060       -0.247470   \n",
       "13427  -0.888344        0.267501        0.014950       -0.037578   \n",
       "13428  -0.886517        0.435141       -0.141346        0.172410   \n",
       "13429  -0.883750        0.568520       -0.276652        0.327679   \n",
       "13430  -0.880309        0.625730       -0.415188        0.417500   \n",
       "13431  -0.876573        0.603726       -0.512972        0.422644   \n",
       "13432  -0.872868        0.535731       -0.561337        0.392541   \n",
       "13433  -0.869477        0.448781       -0.542783        0.360524   \n",
       "13434  -0.866394        0.375423       -0.552782        0.340226   \n",
       "13435  -0.863346        0.330595       -0.602148        0.302538   \n",
       "13436  -0.860490        0.299249       -0.541737        0.273743   \n",
       "13437  -0.857953        0.277268       -0.450595        0.252545   \n",
       "13438  -0.855693        0.262801       -0.372523        0.211931   \n",
       "13439  -0.853639        0.249375       -0.318617        0.175547   \n",
       "13440  -0.851743        0.227398       -0.298865        0.162747   \n",
       "13441  -0.849944        0.194780       -0.325347        0.173501   \n",
       "13442  -0.848005        0.193589       -0.393524        0.200196   \n",
       "13443  -0.846084        0.082093       -0.468608        0.230351   \n",
       "\n",
       "       userAcceleration.x  userAcceleration.y  userAcceleration.z  partc  \\\n",
       "0                0.118248            0.071449           -0.070273      1   \n",
       "1                0.117502            0.042686            0.038011      1   \n",
       "2                0.103520            0.058405            0.177809      1   \n",
       "3                0.072650            0.022306            0.264677      1   \n",
       "4               -0.014910           -0.006723            0.293112      1   \n",
       "5               -0.050790            0.031104            0.224405      1   \n",
       "6               -0.032926            0.112645            0.153538      1   \n",
       "7                0.027988            0.159221            0.006546      1   \n",
       "8                0.098329            0.103732           -0.231075      1   \n",
       "9                0.114100            0.047566           -0.373146      1   \n",
       "10               0.026935            0.062557           -0.373894      1   \n",
       "11              -0.046481            0.134258           -0.298300      1   \n",
       "12               0.041178            0.171593           -0.161313      1   \n",
       "13               0.174746            0.099403           -0.027149      1   \n",
       "14               0.172982            0.095715            0.097081      1   \n",
       "15               0.122255            0.145908            0.169496      1   \n",
       "16               0.072518            0.077136            0.104327      1   \n",
       "17              -0.034353            0.019040           -0.025478      1   \n",
       "18              -0.094807            0.024185           -0.090437      1   \n",
       "19              -0.103218            0.019623           -0.104743      1   \n",
       "20              -0.080853            0.005062           -0.095309      1   \n",
       "21              -0.054467           -0.012035           -0.106802      1   \n",
       "22               0.050042           -0.016996           -0.072191      1   \n",
       "23               0.087532            0.080649            0.162891      1   \n",
       "24               0.173909            0.159802            0.387647      1   \n",
       "25               0.265682            0.171140            0.282955      1   \n",
       "26               0.208749            0.160681            0.065868      1   \n",
       "27               0.151388            0.104343           -0.047408      1   \n",
       "28               0.097157            0.076102           -0.095545      1   \n",
       "29               0.063067            0.076929           -0.122467      1   \n",
       "...                   ...                 ...                 ...    ...   \n",
       "13414           -0.003896            0.081582            0.014021      1   \n",
       "13415           -0.058976            0.088989            0.062938      1   \n",
       "13416           -0.091025            0.082106            0.081195      1   \n",
       "13417           -0.091715            0.079567            0.081615      1   \n",
       "13418           -0.069695            0.083987            0.076117      1   \n",
       "13419           -0.031910            0.074650            0.054961      1   \n",
       "13420           -0.001541            0.061069            0.032087      1   \n",
       "13421           -0.099254            0.064668            0.020818      1   \n",
       "13422           -0.321139            0.082780           -0.008615      1   \n",
       "13423           -0.466380            0.060509           -0.048938      1   \n",
       "13424           -0.403215            0.015539            0.022261      1   \n",
       "13425           -0.263209            0.028507            0.078281      1   \n",
       "13426           -0.125599            0.028918            0.025342      1   \n",
       "13427           -0.003423           -0.027354           -0.051033      1   \n",
       "13428            0.048053           -0.062155           -0.092212      1   \n",
       "13429            0.025232           -0.044863           -0.086541      1   \n",
       "13430           -0.022208           -0.011313           -0.043779      1   \n",
       "13431           -0.063868            0.016420            0.012925      1   \n",
       "13432           -0.071381            0.039611            0.052266      1   \n",
       "13433           -0.049908            0.057200            0.071275      1   \n",
       "13434           -0.025528            0.050379            0.064133      1   \n",
       "13435           -0.009903            0.044499            0.032856      1   \n",
       "13436            0.009214            0.055047            0.028275      1   \n",
       "13437            0.006634            0.054727            0.029523      1   \n",
       "13438           -0.005585            0.047703            0.027248      1   \n",
       "13439           -0.017820            0.041790            0.034105      1   \n",
       "13440           -0.021793            0.035129            0.037595      1   \n",
       "13441           -0.026456            0.026596            0.034865      1   \n",
       "13442           -0.030990            0.015427            0.008832      1   \n",
       "13443           -0.034977            0.010196            0.069900      1   \n",
       "\n",
       "      action  action_file_index  \n",
       "0        ups                  1  \n",
       "1        ups                  1  \n",
       "2        ups                  1  \n",
       "3        ups                  1  \n",
       "4        ups                  1  \n",
       "5        ups                  1  \n",
       "6        ups                  1  \n",
       "7        ups                  1  \n",
       "8        ups                  1  \n",
       "9        ups                  1  \n",
       "10       ups                  1  \n",
       "11       ups                  1  \n",
       "12       ups                  1  \n",
       "13       ups                  1  \n",
       "14       ups                  1  \n",
       "15       ups                  1  \n",
       "16       ups                  1  \n",
       "17       ups                  1  \n",
       "18       ups                  1  \n",
       "19       ups                  1  \n",
       "20       ups                  1  \n",
       "21       ups                  1  \n",
       "22       ups                  1  \n",
       "23       ups                  1  \n",
       "24       ups                  1  \n",
       "25       ups                  1  \n",
       "26       ups                  1  \n",
       "27       ups                  1  \n",
       "28       ups                  1  \n",
       "29       ups                  1  \n",
       "...      ...                ...  \n",
       "13414    std                 18  \n",
       "13415    std                 18  \n",
       "13416    std                 18  \n",
       "13417    std                 18  \n",
       "13418    std                 18  \n",
       "13419    std                 18  \n",
       "13420    std                 18  \n",
       "13421    std                 18  \n",
       "13422    std                 18  \n",
       "13423    std                 18  \n",
       "13424    std                 18  \n",
       "13425    std                 18  \n",
       "13426    std                 18  \n",
       "13427    std                 18  \n",
       "13428    std                 18  \n",
       "13429    std                 18  \n",
       "13430    std                 18  \n",
       "13431    std                 18  \n",
       "13432    std                 18  \n",
       "13433    std                 18  \n",
       "13434    std                 18  \n",
       "13435    std                 18  \n",
       "13436    std                 18  \n",
       "13437    std                 18  \n",
       "13438    std                 18  \n",
       "13439    std                 18  \n",
       "13440    std                 18  \n",
       "13441    std                 18  \n",
       "13442    std                 18  \n",
       "13443    std                 18  \n",
       "\n",
       "[165965 rows x 15 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_smooth(predictions):\n",
    "    most_common = max(set(predictions), key=predictions.count)\n",
    "    return [most_common] * len(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
