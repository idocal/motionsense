# MotiononSense Dataset
### Problem definition: predict user's activity based on smartphone sensors data

---
## Overview

Our main goal is to analyze how analytical data from mobile sensors can be used to identify what a user is actually doing in real-life. We first downloaded a dataset from Kaggle and chose the best performing model. Then, we gathered our own data and checked if the model still evaluates well. Next, we developed our own mobile app to see our model in action. Finally, we used a boosting method to improve real life predictions. Our point of focus here is to demonstrate the whole cycle: How a data science analysis of an outsourced dataset turns into a working product.

---
## Business Understating

### Some context about how the data was created:

* This dataset includes time-series data generated by accelerometer and gyroscope sensors
* It is collected with an iPhone 6s kept in the participant's front pocket
* A total of 24 participants performed 6 activities in 15 trials in the same environment and conditions.
* The activities are divided to : downstairs, upstairs, walking, jogging, sitting, and standing.

The content of the data set is divided to two types of trails:

* Long trials: those with number 1 to 9 with around 2 to 3 minutes duration.
* Short trials: those with number 11 to 16 that are around 30 seconds to 1 minutes duration

The dataset contains time-series collected by both Accelerometer and Gyroscope

* For every trial we have a multivariate time-series
* Thus, we have time-series with a total of 12 features, 3 axis for each measurement : Attitude, Gravity, Rotation Rate and User Acceleration

---
## Problem Definition and Applications

* Our probelm is predicting user's activity from phone sensors data
* This definition might be too wide, so we limit ourself to predicting 1 of 5 possible activities
* Thus, we can define our problem as multiclass classification, where we can label each data point as <br> sitting, standing, walking, going downstaris or going upstairs
* There are many application for this kind of classification in various fields such as <br> healthcare, intelligence etc.
* We will further discuss some of these applications in later part of our project

---
## Feature Extraction / Engineering

Our data is a time series - a sequence of measurements over time

* Thus, extracting value for a single data point depends on it's context
* But, classic ML algorithms/classifiers predicts output for a single input data point - independent to adjacent input data point
* So, in order to use our data to train classic ML model we will have to encode our features to represent context data
* We will present two different features encoding methods - Sliding-Window and Raw-History

### Sliding Window Features

* In this method, we will encode each data sample as a concatenation of analytical functions calculated over a predefined size of previous samples
* For example, here we will use a context size of 10 (calculate over 10 pervious data points)
* Notice that we cannot mix between different experiments who represents different activity labels

### Raw History Features

* In this method, we will simply encode each data sample as a concatenation of the raw features of it's previous x data points
* For example, here we will use a context size of 10. i.e it is aligned with our previous sliding window method, <br> but instead of calculating aggregation of analytical function over the context features, here we simply encode them as a long vector
* Again, we cannot mix between different experiments who represents different activity label

---
## Classic ML Models - Training and Statistical Evaluation

* Now we have two different encodings of our time series data as independent data points.
* We can feed them into an ML model and evaluate the performance of our predictions
* First we will define a class to consolidate the functionality of splitting the data into training, development and test sets <br> and performing the evaluation using precision and recall for each activity label

**A few words about our evaluation metrics**

* Our evaluation metrics will be precision, recall and their harmonic average the F1 score
* These metrics are much more relevant to our problem compared to the model total accuracy for few reasons.
* First, our problem is imbalanced. We've already seen that the labels walk sit and stand are x2 time more frequent than going up/down stairs
* Second, We also consider our assumption that some activities will be much harder to predict compared to others. <br> i.e separating "sit" from "walk" should be much easier than separating between "upstairs" and "downstairs"
* That is why we'll be interested in the model performance for each activity by its own.

---
## Problems and the Need for "Real Data"

* As we can see, even on our left aside test data the performance of the model is too good to be true.
* This is a reason to suspect that although we tested our model on data was not used to train it, **we are over-fitting to the current data set as a whole**
* We suspect that **the generation process of the data was too "synthetic"**, not representing "real world" data obtained from phone sensors
* We used this experiment framework to **extract real data obtained from our activities** during the day and labeled them accordingly
* In the next notebook we will present our model performance on our "real world" data and try to train more complex models to improve our performance over the real world data
